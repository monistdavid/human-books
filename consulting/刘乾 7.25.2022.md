Question
===============

1. What is the best performance retrieval model?
    1. with the generative model, still need retrieval model?
2. finetune the GPT model, or adding a persona embedding will give better result?
    1. persona embedding might now be able to generate good embedding to represent the user data that are not showing in
       the training dataset, is it true?
    2. Books character might have a lot of different perspectives.
3. why multi-head transformers could capture more features of an input data? Why the large amount of parameters could
   not capture those features?
4. what is the advantage of using log function?
5. https://arxiv.org/pdf/1510.03055.pdf mentioned that As decoding proceeds, the influence of the initial input on
   decoding (i.e., the source sentence representation) diminishes as additional previously-predicted words are encoded
   in the vector representations. Is this the problem only in RNN or also in Transformers. If we are using Transformers,
   the issue that MMI penalizes not only high-frequency, generic responses, but also fluent ones and thus can lead to
   ungrammatical outputs maybe is not an issue anymore?
6. different between parameters and hidden neurons. How to calculate the size of a model.
7. 

Notes
===============



Thoughts with Additional Information
===============

