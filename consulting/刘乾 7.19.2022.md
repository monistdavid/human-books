Question
===============

1. when is GPU needed and when is CPU needed?
    1. GPU is needed for training, is it needed for inference as well? (GPT Pretrained language model?)
    2. when the thread is generated and used to process the program. Can GPU be used in parallel processing? If not, why
       CPU can be used in parallel processing?
    3. talks about the NLP Cloud, how they work
2. If you were me, how would you start to do this project?
3. GPT is really versatile. If I Finetune GPT-J model with custom data, how much will I decrease its versatility? In
   other word, will it only reply response with what the custom data has? Or it still retains the knowledge from the GPT
   LM? What makes things different with fine-tuning blenderbot with GPT LM?
4. How serious will the uppercase lowercase influence the model
5. things to pay attention when doing finetune.
6. Cost for production or deployment
7. what account good data processing, what kind of data is well processed
8. what do you think of prompt based learning.

Notes
===============

1. GPU could definitely run model quicker, however, it cost a lot more to rent GPU RAM than CPU RAM.
    1. so larger model with low inference time should be used on CPU, smaller model with high inference time should be
       used on GPU. As this could balance the inference time of the program.
2. what to pay attention for Fine-tune in case that to fine-tune dataset is a lot different from the pretrained dataset.
    1. learning rate needs to be smaller than e-5
    2. input/output format needs to be the same as the pretrained model. (not only the format, but maybe the length of
       the data, the maximum number of turns in the dialogue dataset.)
    3. learning step/epoch, needs to be careful about model's quickly over-fitting
3. feedback system
    1. the ability to quickly update
4. two types of generating algorithm
    1. greedy decoding
        1. always select the highest possibility for each word.
        2. beam search is a modified version of greedy decoding. Instead of always selecting the highest possibility for
           each word, choose the top k word for each word selection, get the conditional probability for each turn and
           get the top k again.
    2. sampling
        1. top k sampling. Randomly choose from the top k words for each turn
        2. threshold sampling. Randomly choose from the words that have possibilities that are beyond certain threshold.
        3. nuclear sampling. Fro each turn, select k words, each with certain possibilities, once total possibilities
           sums up to a certain number, then stop selecting.
5. distill
    1. decrease the parameters by 50% with only 1% of performance decreasing.
6. De-generate
    1. constantly replicate certain word or sentences. For example, it will repeat itself "Hi, how are you. Hi, how are
       you".
    2. 

Thoughts
===============