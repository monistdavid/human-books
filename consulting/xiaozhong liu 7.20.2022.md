Question
===============

1. what do you think of the technique challenges part and the easy to solve part?
2. What is the best performance retrieval model?
3. store user memory and dialogue history usage
4. GPT is really versatile. If I Finetune GPT-J model with custom data, how much will I decrease its versatility? In
   other word, will it only reply response with what the custom data has? Or it still retains the knowledge from the GPT
   LM? What makes things different with fine-tuning blenderbot with GPT LM?
5. finetune needs to pay attention of learning rate, input/output formate and step/epoch. Is there a way to measure the
   exact numbers of each?
6. how to really relate the character story to the dialogue. For example, when I talk about my bad relationship with my
   mom, the chatbot from "The Stationery Store" might say "I remember I have bad relationship with my grandma....",
   instead of saying "I am sorry to hear"
7. persona consistency
8. deployment, production
9. how do you prepare to do this project

Notes
===============



Thoughts
===============

