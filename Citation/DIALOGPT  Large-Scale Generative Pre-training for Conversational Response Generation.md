Link
===============
<p>

https://arxiv.org/pdf/1911.00536.pdf

</p>


Notes
===============

1. GPT2 have the capacity to capture textual data with fine granularity and produce output with a high-resolution that
   closely emulates real-world text written by humans.

Thoughts
===============

1. DIALOGPT is trained on 147M conversation-like exchanges from reddit? 
   1. why not directly train on conversational dialogue?
2. What kind of information or abilities does language models have?
   1. Does language model understand words relationships or sentence relationships?
      For example, if I have context about Harry Potter, does the language models
      generate sentences related to harry potter in both the language style and 
      the related context relationship? Or it can only generate a making sense 
      sentences.

Summary
===============