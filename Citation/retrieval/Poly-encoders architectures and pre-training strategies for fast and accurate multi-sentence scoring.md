Link    
===============
<p>

https://arxiv.org/pdf/1905.01969.pdf

</p>


Notes
===============
1. For tasks that make pairwise comparisons between sequences, matching a given input with a
   corresponding label, two approaches are common: Cross-encoders performing full self-attention over the
   pair and Bi-encoders encoding the pair separately. The former often performs
   better, but is too slow for practical use. In this work, we develop a new transformer architecture, 
   the Poly-encoder, that learns global rather than token level self-attention features. 







Thoughts
===============



Summary
===============