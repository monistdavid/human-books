Link    
===============
<p>

https://arxiv.org/pdf/2204.02311.pdf

</p>


Notes
===============
1. Although these models have achieved near universal state of the art across thousands of natural language tasks, 
   the downside is that they require a significant number of task-specific training examples to finetune the model. 
   Additionally, at least a portion of the model parameters must be updated to fit the task,
   adding complexity from model finetuning and deployment.
2. The improvements in these models have primarily come from one or more of the following approaches:
   1. scaling the size of the models in both depth and width
   2. increasing the number of tokens that the model was trained on
   3. training on cleaner datasets from more diverse sources
   4. increasing model capacity without increasing the computational cost through sparsely activated modules.
   
Thoughts
===============

Summary   
===============